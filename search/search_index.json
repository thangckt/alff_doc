{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>alff</code> Documentation","text":""},{"location":"#alff","title":"<code>alff</code>","text":"<p>Framework for performing Active Learning generation of ML-based Forcefields.</p> <p>This package is developed and maintained by C.Thang Nguyen</p>"},{"location":"alff_info/","title":"alff parameters","text":""},{"location":"alff_info/#alff.lib.arginfo","title":"<code>alff.lib.arginfo</code>","text":"<p><code>alff</code> accepts a configuration file in YAML/JSON/JSONC format.</p>"},{"location":"alff_info/#alff.lib.arginfo.param","title":"<code>param()</code>","text":"<p>ALFF parameters.</p>"},{"location":"alff_info/#alff.lib.arginfo.not_use_param_seveen","title":"<code>not_use_param_seveen()</code>","text":"<p>SevenNet parameters that are not applicable.</p> <p>These parameters are either generated by the ALFF or are not required for running the ALFF.</p>"},{"location":"alff_info/#alff.lib.arginfo.machine","title":"<code>machine()</code>","text":"<p>ALFF parameters for running on a clusters.</p>"},{"location":"mace_train_param/","title":"MACE train","text":""},{"location":"mace_train_param/#mace-parameters-for-training","title":"MACE parameters for training","text":"<p>See more development parameters here.</p>"},{"location":"mace_train_param/#name-and-seed","title":"Name and seed","text":"<pre><code>    parser.add_argument(\"--name\", help=\"experiment name\", required=True)\n    parser.add_argument(\"--seed\", help=\"random seed\", type=int, default=123)\n</code></pre>"},{"location":"mace_train_param/#directories","title":"Directories","text":"<pre><code>    parser.add_argument(\n        \"--work_dir\",\n        help=\"set directory for all files and folders\",\n        type=str,\n        default=\".\",\n    )\n    parser.add_argument(\n        \"--log_dir\", help=\"directory for log files\", type=str, default=None\n    )\n    parser.add_argument(\n        \"--model_dir\", help=\"directory for final model\", type=str, default=None\n    )\n    parser.add_argument(\n        \"--checkpoints_dir\",\n        help=\"directory for checkpoint files\",\n        type=str,\n        default=None,\n    )\n    parser.add_argument(\n        \"--results_dir\", help=\"directory for results\", type=str, default=None\n    )\n    parser.add_argument(\n        \"--downloads_dir\", help=\"directory for downloads\", type=str, default=None\n    )\n\n    ## Device and logging\n    parser.add_argument(\n        \"--device\",\n        help=\"select device\",\n        type=str,\n        choices=[\"cpu\", \"cuda\", \"mps\", \"xpu\"],\n        default=\"cpu\",\n    )\n    parser.add_argument(\n        \"--default_dtype\",\n        help=\"set default dtype\",\n        type=str,\n        choices=[\"float32\", \"float64\"],\n        default=\"float64\",\n    )\n    parser.add_argument(\n        \"--distributed\",\n        help=\"train in multi-GPU data parallel mode\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\"--log_level\", help=\"log level\", type=str, default=\"INFO\")\n\n    parser.add_argument(\n        \"--error_table\",\n        help=\"Type of error table produced at the end of the training\",\n        type=str,\n        choices=[\n            \"PerAtomRMSE\",\n            \"TotalRMSE\",\n            \"PerAtomRMSEstressvirials\",\n            \"PerAtomMAEstressvirials\",\n            \"PerAtomMAE\",\n            \"TotalMAE\",\n            \"DipoleRMSE\",\n            \"DipoleMAE\",\n            \"EnergyDipoleRMSE\",\n        ],\n        default=\"PerAtomRMSE\",\n    )\n</code></pre>"},{"location":"mace_train_param/#model","title":"Model","text":"<pre><code>    parser.add_argument(\n        \"--model\",\n        help=\"model type\",\n        default=\"MACE\",\n        choices=[\n            \"BOTNet\",\n            \"MACE\",\n            \"ScaleShiftMACE\",\n            \"ScaleShiftBOTNet\",\n            \"AtomicDipolesMACE\",\n            \"EnergyDipolesMACE\",\n        ],\n    )\n    parser.add_argument(\n        \"--r_max\", help=\"distance cutoff (in Ang)\", type=float, default=5.0\n    )\n    parser.add_argument(\n        \"--radial_type\",\n        help=\"type of radial basis functions\",\n        type=str,\n        default=\"bessel\",\n        choices=[\"bessel\", \"gaussian\", \"chebyshev\"],\n    )\n    parser.add_argument(\n        \"--num_radial_basis\",\n        help=\"number of radial basis functions\",\n        type=int,\n        default=8,\n    )\n    parser.add_argument(\n        \"--num_cutoff_basis\",\n        help=\"number of basis functions for smooth cutoff\",\n        type=int,\n        default=5,\n    )\n    parser.add_argument(\n        \"--pair_repulsion\",\n        help=\"use pair repulsion term with ZBL potential\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--distance_transform\",\n        help=\"use distance transform for radial basis functions\",\n        default=\"None\",\n        choices=[\"None\", \"Agnesi\", \"Soft\"],\n    )\n    parser.add_argument(\n        \"--interaction\",\n        help=\"name of interaction block\",\n        type=str,\n        default=\"RealAgnosticResidualInteractionBlock\",\n        choices=[\n            \"RealAgnosticResidualInteractionBlock\",\n            \"RealAgnosticAttResidualInteractionBlock\",\n            \"RealAgnosticInteractionBlock\",\n        ],\n    )\n    parser.add_argument(\n        \"--interaction_first\",\n        help=\"name of interaction block\",\n        type=str,\n        default=\"RealAgnosticResidualInteractionBlock\",\n        choices=[\n            \"RealAgnosticResidualInteractionBlock\",\n            \"RealAgnosticInteractionBlock\",\n        ],\n    )\n    parser.add_argument(\n        \"--max_ell\", help=r\"highest \\ell of spherical harmonics\", type=int, default=3\n    )\n    parser.add_argument(\n        \"--correlation\", help=\"correlation order at each layer\", type=int, default=3\n    )\n    parser.add_argument(\n        \"--num_interactions\", help=\"number of interactions\", type=int, default=2\n    )\n    parser.add_argument(\n        \"--MLP_irreps\",\n        help=\"hidden irreps of the MLP in last readout\",\n        type=str,\n        default=\"16x0e\",\n    )\n    parser.add_argument(\n        \"--radial_MLP\",\n        help=\"width of the radial MLP\",\n        type=str,\n        default=\"[64, 64, 64]\",\n    )\n    parser.add_argument(\n        \"--hidden_irreps\",\n        help=\"irreps for hidden node states\",\n        type=str,\n        default=None,\n    )\n    ## add option to specify irreps by channel number and max L\n    parser.add_argument(\n        \"--num_channels\",\n        help=\"number of embedding channels\",\n        type=int,\n        default=None,\n    )\n    parser.add_argument(\n        \"--max_L\",\n        help=\"max L equivariance of the message\",\n        type=int,\n        default=None,\n    )\n    parser.add_argument(\n        \"--gate\",\n        help=\"non linearity for last readout\",\n        type=str,\n        default=\"silu\",\n        choices=[\"silu\", \"tanh\", \"abs\", \"None\"],\n    )\n    parser.add_argument(\n        \"--scaling\",\n        help=\"type of scaling to the output\",\n        type=str,\n        default=\"rms_forces_scaling\",\n        choices=[\"std_scaling\", \"rms_forces_scaling\", \"no_scaling\"],\n    )\n    parser.add_argument(\n        \"--avg_num_neighbors\",\n        help=\"normalization factor for the message\",\n        type=float,\n        default=1,\n    )\n    parser.add_argument(\n        \"--compute_avg_num_neighbors\",\n        help=\"normalization factor for the message\",\n        type=str2bool,\n        default=True,\n    )\n    parser.add_argument(\n        \"--compute_stress\",\n        help=\"Select True to compute stress\",\n        type=str2bool,\n        default=False,\n    )\n    parser.add_argument(\n        \"--compute_forces\",\n        help=\"Select True to compute forces\",\n        type=str2bool,\n        default=True,\n    )\n</code></pre>"},{"location":"mace_train_param/#dataset","title":"Dataset","text":"<pre><code>    parser.add_argument(\n        \"--train_file\",\n        help=\"Training set file, format is .xyz or .h5\",\n        type=str,\n        required=False,\n    )\n    parser.add_argument(\n        \"--valid_file\",\n        help=\"Validation set .xyz or .h5 file\",\n        default=None,\n        type=str,\n        required=False,\n    )\n    parser.add_argument(\n        \"--valid_fraction\",\n        help=\"Fraction of training set used for validation\",\n        type=float,\n        default=0.1,\n        required=False,\n    )\n    parser.add_argument(\n        \"--test_file\",\n        help=\"Test set .xyz pt .h5 file\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--test_dir\",\n        help=\"Path to directory with test files named as test_*.h5\",\n        type=str,\n        default=None,\n        required=False,\n    )\n    parser.add_argument(\n        \"--multi_processed_test\",\n        help=\"Boolean value for whether the test data was multiprocessed\",\n        type=str2bool,\n        default=False,\n        required=False,\n    )\n    parser.add_argument(\n        \"--num_workers\",\n        help=\"Number of workers for data loading\",\n        type=int,\n        default=0,\n    )\n    parser.add_argument(\n        \"--pin_memory\",\n        help=\"Pin memory for data loading\",\n        default=True,\n        type=str2bool,\n    )\n    parser.add_argument(\n        \"--atomic_numbers\",\n        help=\"List of atomic numbers\",\n        type=str,\n        default=None,\n        required=False,\n    )\n    parser.add_argument(\n        \"--mean\",\n        help=\"Mean energy per atom of training set\",\n        type=float,\n        default=None,\n        required=False,\n    )\n    parser.add_argument(\n        \"--std\",\n        help=\"Standard deviation of force components in the training set\",\n        type=float,\n        default=None,\n        required=False,\n    )\n    parser.add_argument(\n        \"--statistics_file\",\n        help=\"json file containing statistics of training set\",\n        type=str,\n        default=None,\n        required=False,\n    )\n    parser.add_argument(\n        \"--E0s\",\n        help=\"Dictionary of isolated atom energies\",\n        type=str,\n        default=None,\n        required=False,\n    )\n\n\n\n    ## Keys\n    parser.add_argument(\n        \"--energy_key\",\n        help=\"Key of reference energies in training xyz\",\n        type=str,\n        default=\"REF_energy\",\n    )\n    parser.add_argument(\n        \"--forces_key\",\n        help=\"Key of reference forces in training xyz\",\n        type=str,\n        default=\"REF_forces\",\n    )\n    parser.add_argument(\n        \"--virials_key\",\n        help=\"Key of reference virials in training xyz\",\n        type=str,\n        default=\"REF_virials\",\n    )\n    parser.add_argument(\n        \"--stress_key\",\n        help=\"Key of reference stress in training xyz\",\n        type=str,\n        default=\"REF_stress\",\n    )\n    parser.add_argument(\n        \"--dipole_key\",\n        help=\"Key of reference dipoles in training xyz\",\n        type=str,\n        default=\"REF_dipole\",\n    )\n    parser.add_argument(\n        \"--charges_key\",\n        help=\"Key of atomic charges in training xyz\",\n        type=str,\n        default=\"REF_charges\",\n    )\n</code></pre>"},{"location":"mace_train_param/#fine-tuning","title":"Fine-tuning","text":"<pre><code>    parser.add_argument(\n        \"--foundation_filter_elements\",\n        help=\"Filter element during fine-tuning\",\n        type=str2bool,\n        default=True,\n        required=False,\n    )\n    parser.add_argument(\n        \"--heads\",\n        help=\"Dict of heads: containing individual files and E0s\",\n        type=str,\n        default=None,\n        required=False,\n    )\n    parser.add_argument(\n        \"--multiheads_finetuning\",\n        help=\"Boolean value for whether the model is multiheaded\",\n        type=str2bool,\n        default=True,\n    )\n    parser.add_argument(\n        \"--weight_pt_head\",\n        help=\"Weight of the pretrained head in the loss function\",\n        type=float,\n        default=1.0,\n    )\n    parser.add_argument(\n        \"--num_samples_pt\",\n        help=\"Number of samples in the pretrained head\",\n        type=int,\n        default=1000,\n    )\n    parser.add_argument(\n        \"--subselect_pt\",\n        help=\"Method to subselect the configurations of the pretraining set\",\n        choices=[\"fps\", \"random\"],\n        default=\"random\",\n    )\n    parser.add_argument(\n        \"--pt_train_file\",\n        help=\"Training set file for the pretrained head\",\n        type=str,\n        default=None,\n    )\n    parser.add_argument(\n        \"--pt_valid_file\",\n        help=\"Validation set file for the pretrained head\",\n        type=str,\n        default=None,\n    )\n    parser.add_argument(\n        \"--keep_isolated_atoms\",\n        help=\"Keep isolated atoms in the dataset, useful for transfer learning\",\n        type=str2bool,\n        default=False,\n    )\n</code></pre>"},{"location":"mace_train_param/#loss-and-optimization","title":"Loss and optimization","text":"<pre><code>    parser.add_argument(\n        \"--loss\",\n        help=\"type of loss\",\n        default=\"weighted\",\n        choices=[\n            \"ef\",\n            \"weighted\",\n            \"forces_only\",\n            \"virials\",\n            \"stress\",\n            \"dipole\",\n            \"huber\",\n            \"universal\",\n            \"energy_forces_dipole\",\n        ],\n    )\n    parser.add_argument(\n        \"--forces_weight\", help=\"weight of forces loss\", type=float, default=100.0\n    )\n    parser.add_argument(\n        \"--swa_forces_weight\",\n        \"--stage_two_forces_weight\",\n        help=\"weight of forces loss after starting Stage Two (previously called swa)\",\n        type=float,\n        default=100.0,\n        dest=\"swa_forces_weight\",\n    )\n    parser.add_argument(\n        \"--energy_weight\", help=\"weight of energy loss\", type=float, default=1.0\n    )\n    parser.add_argument(\n        \"--swa_energy_weight\",\n        \"--stage_two_energy_weight\",\n        help=\"weight of energy loss after starting Stage Two (previously called swa)\",\n        type=float,\n        default=1000.0,\n        dest=\"swa_energy_weight\",\n    )\n    parser.add_argument(\n        \"--virials_weight\", help=\"weight of virials loss\", type=float, default=1.0\n    )\n    parser.add_argument(\n        \"--swa_virials_weight\",\n        \"--stage_two_virials_weight\",\n        help=\"weight of virials loss after starting Stage Two (previously called swa)\",\n        type=float,\n        default=10.0,\n        dest=\"swa_virials_weight\",\n    )\n    parser.add_argument(\n        \"--stress_weight\", help=\"weight of virials loss\", type=float, default=1.0\n    )\n    parser.add_argument(\n        \"--swa_stress_weight\",\n        \"--stage_two_stress_weight\",\n        help=\"weight of stress loss after starting Stage Two (previously called swa)\",\n        type=float,\n        default=10.0,\n        dest=\"swa_stress_weight\",\n    )\n    parser.add_argument(\n        \"--dipole_weight\", help=\"weight of dipoles loss\", type=float, default=1.0\n    )\n    parser.add_argument(\n        \"--swa_dipole_weight\",\n        \"--stage_two_dipole_weight\",\n        help=\"weight of dipoles after starting Stage Two (previously called swa)\",\n        type=float,\n        default=1.0,\n        dest=\"swa_dipole_weight\",\n    )\n    parser.add_argument(\n        \"--config_type_weights\",\n        help=\"String of dictionary containing the weights for each config type\",\n        type=str,\n        default='{\"Default\":1.0}',\n    )\n    parser.add_argument(\n        \"--huber_delta\",\n        help=\"delta parameter for huber loss\",\n        type=float,\n        default=0.01,\n    )\n    parser.add_argument(\n        \"--optimizer\",\n        help=\"Optimizer for parameter optimization\",\n        type=str,\n        default=\"adam\",\n        choices=[\"adam\", \"adamw\", \"schedulefree\"],\n    )\n    parser.add_argument(\n        \"--beta\",\n        help=\"Beta parameter for the optimizer\",\n        type=float,\n        default=0.9,\n    )\n    parser.add_argument(\"--batch_size\", help=\"batch size\", type=int, default=10)\n    parser.add_argument(\n        \"--valid_batch_size\", help=\"Validation batch size\", type=int, default=10\n    )\n    parser.add_argument(\n        \"--lr\", help=\"Learning rate of optimizer\", type=float, default=0.01\n    )\n    parser.add_argument(\n        \"--swa_lr\",\n        \"--stage_two_lr\",\n        help=\"Learning rate of optimizer in Stage Two (previously called swa)\",\n        type=float,\n        default=1e-3,\n        dest=\"swa_lr\",\n    )\n    parser.add_argument(\n        \"--weight_decay\", help=\"weight decay (L2 penalty)\", type=float, default=5e-7\n    )\n    parser.add_argument(\n        \"--amsgrad\",\n        help=\"use amsgrad variant of optimizer\",\n        action=\"store_true\",\n        default=True,\n    )\n    parser.add_argument(\n        \"--scheduler\", help=\"Type of scheduler\", type=str, default=\"ReduceLROnPlateau\"\n    )\n    parser.add_argument(\n        \"--lr_factor\", help=\"Learning rate factor\", type=float, default=0.8\n    )\n    parser.add_argument(\n        \"--scheduler_patience\", help=\"Learning rate factor\", type=int, default=50\n    )\n    parser.add_argument(\n        \"--lr_scheduler_gamma\",\n        help=\"Gamma of learning rate scheduler\",\n        type=float,\n        default=0.9993,\n    )\n    parser.add_argument(\n        \"--swa\",\n        \"--stage_two\",\n        help=\"use Stage Two loss weight, which decreases the learning rate and increases the energy weight at the end of the training to help converge them\",\n        action=\"store_true\",\n        default=False,\n        dest=\"swa\",\n    )\n    parser.add_argument(\n        \"--start_swa\",\n        \"--start_stage_two\",\n        help=\"Number of epochs before changing to Stage Two loss weights\",\n        type=int,\n        default=None,\n        dest=\"start_swa\",\n    )\n    parser.add_argument(\n        \"--ema\",\n        help=\"use Exponential Moving Average\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--ema_decay\",\n        help=\"Exponential Moving Average decay\",\n        type=float,\n        default=0.99,\n    )\n    parser.add_argument(\n        \"--max_num_epochs\", help=\"Maximum number of epochs\", type=int, default=2048\n    )\n    parser.add_argument(\n        \"--patience\",\n        help=\"Maximum number of consecutive epochs of increasing loss\",\n        type=int,\n        default=2048,\n    )\n    parser.add_argument(\n        \"--foundation_model\",\n        help=\"Path to the foundation model for transfer learning\",\n        type=str,\n        default=None,\n    )\n    parser.add_argument(\n        \"--foundation_model_readout\",\n        help=\"Use readout of foundation model for transfer learning\",\n        action=\"store_false\",\n        default=True,\n    )\n    parser.add_argument(\n        \"--eval_interval\", help=\"evaluate model every &lt;n&gt; epochs\", type=int, default=1\n    )\n    parser.add_argument(\n        \"--keep_checkpoints\",\n        help=\"keep all checkpoints\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--save_all_checkpoints\",\n        help=\"save all checkpoints\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--restart_latest\",\n        help=\"restart optimizer from latest checkpoint\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--save_cpu\",\n        help=\"Save a model to be loaded on cpu\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--clip_grad\",\n        help=\"Gradient Clipping Value\",\n        type=check_float_or_none,\n        default=10.0,\n    )\n    ## options for using Weights and Biases for experiment tracking\n    ## to install see https://wandb.ai\n    parser.add_argument(\n        \"--wandb\",\n        help=\"Use Weights and Biases for experiment tracking\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--wandb_dir\",\n        help=\"An absolute path to a directory where Weights and Biases metadata will be stored\",\n        type=str,\n        default=None,\n    )\n    parser.add_argument(\n        \"--wandb_project\",\n        help=\"Weights and Biases project name\",\n        type=str,\n        default=\"\",\n    )\n    parser.add_argument(\n        \"--wandb_entity\",\n        help=\"Weights and Biases entity name\",\n        type=str,\n        default=\"\",\n    )\n    parser.add_argument(\n        \"--wandb_name\",\n        help=\"Weights and Biases experiment name\",\n        type=str,\n        default=\"\",\n    )\n    parser.add_argument(\n        \"--wandb_log_hypers\",\n        help=\"The hyperparameters to log in Weights and Biases\",\n        type=list,\n        default=[\n            \"num_channels\",\n            \"max_L\",\n            \"correlation\",\n            \"lr\",\n            \"swa_lr\",\n            \"weight_decay\",\n            \"batch_size\",\n            \"max_num_epochs\",\n            \"start_swa\",\n            \"energy_weight\",\n            \"forces_weight\",\n        ],\n    )\n    return parser\n</code></pre>"},{"location":"sevenn_train_param/","title":"SevenNet parameters for training model","text":"<p>There are preset parameters:</p> <ul> <li>base</li> <li>fine_tune</li> <li>sevennet-0</li> </ul> <p>See full full development parameters here.</p> <p>To access energy, force, and stress, in <code>extxyz</code> dataset, the following keys are used: <pre><code>data:\n    data_format_args:\n        energy_key: 'TotEnergy'\n        force_key: 'force'\n        stress_key: 'stress'\n</code></pre></p>"},{"location":"sevenn_train_param/#base","title":"base","text":"<pre><code># Example input.yaml for training SevenNet.\n# '*' signifies default. You can check log.sevenn for defaults.\n\nmodel:\n    chemical_species: 'Auto'                      # Chemical elements present in the dataset, guess them from load_dataset data if 'auto'\n    cutoff: 5.0                                   # Cutoff radius in Angstroms. If two atoms are within the cutoff, they are connected.\n    channel: 32                                   # The multiplicity(channel) of node features.\n    lmax: 2                                       # Maximum order of irreducible representations (rotation order).\n    num_convolution_layer: 3                      # The number of message passing layers.\n\n    #irreps_manual:                               # Manually set irreps of the model in each layer\n        #- \"128x0e\"\n        #- \"128x0e+64x1e+32x2e\"\n        #- \"128x0e+64x1e+32x2e\"\n        #- \"128x0e+64x1e+32x2e\"\n        #- \"128x0e+64x1e+32x2e\"\n        #- \"128x0e\"\n\n    weight_nn_hidden_neurons: [64, 64]            # Hidden neurons in convolution weight neural network\n    radial_basis:                                 # Function and its parameters to encode radial distance\n        radial_basis_name: 'bessel'               # Only 'bessel' is currently supported\n        bessel_basis_num: 8\n    cutoff_function:                              # Envelop function, multiplied to radial_basis functions to init edge featrues\n        cutoff_function_name: 'poly_cut'          # {'poly_cut' and 'poly_cut_p_value'} or {'XPLOR' and 'cutoff_on'}\n        poly_cut_p_value: 6\n\n    act_gate: {'e': 'silu', 'o': 'tanh'}          # Equivalent to 'nonlinearity_gates' in nequip\n    act_scalar: {'e': 'silu', 'o': 'tanh'}        # Equivalent to 'nonlinearity_scalars' in nequip\n\n    is_parity: False                              # Pairy True (E(3) group) or False (to SE(3) group)\n\n    self_connection_type: 'nequip'                # Default is 'nequip'. 'linear' is used for SevenNet-0.\n\n    conv_denominator: \"avg_num_neigh\"             # Valid options are \"avg_num_neigh*\", \"sqrt_avg_num_neigh\", or float\n    train_denominator: False                      # Enable training for denominator in convolution layer\n    train_shift_scale: False                      # Enable training for shift &amp; scale in output layer\n\ntrain:\n    random_seed: 1\n    is_train_stress: True                         # Includes stress in the loss function\n    epoch: 200                                    # Ends training after this number of epochs\n\n    #loss: 'Huber'                                # Default is 'mse' (mean squared error)\n    #loss_param:\n        #delta: 0.01\n\n    # Each optimizer and scheduler have different available parameters.\n    # You can refer to sevenn/train/optim.py for supporting optimizer &amp; schedulers\n    optimizer: 'adam'                             # Options available are 'sgd', 'adagrad', 'adam', 'adamw', 'radam'\n    optim_param:\n        lr: 0.005\n    scheduler: 'exponentiallr'                    # 'steplr', 'multisteplr', 'exponentiallr', 'cosineannealinglr', 'reducelronplateau', 'linearlr'\n    scheduler_param:\n        gamma: 0.99\n\n    force_loss_weight: 0.1                        # Coefficient for force loss\n    stress_loss_weight: 1e-06                     # Coefficient for stress loss (to kbar unit)\n\n    per_epoch: 10                                 # Generate checkpoints every this epoch\n\n    # ['target y', 'metric']\n    # Target y: TotalEnergy, Energy, Force, Stress, Stress_GPa, TotalLoss\n    # Metric  : RMSE, MAE, or Loss\n    error_record:\n        - ['Energy', 'RMSE']\n        - ['Force', 'RMSE']\n        - ['Stress', 'RMSE']\n        - ['TotalLoss', 'None']\n\n    # Continue training model from given checkpoint, or pre-trained model checkpoint for fine-tuning\n    #continue:\n        #checkpoint: 'checkpoint_best.pth'         # Checkpoint of pre-trained model or a model want to continue training.\n        #reset_optimizer: False                    # Set True for fine-tuning\n        #reset_scheduler: False                    # Set True for fine-tuning\n        #use_statistic_values_of_checkpoint: False # Set True to use shift, scale, and avg_num_neigh from checkpoint or not\n\ndata:\n    batch_size: 4                                 # Per GPU batch size.\n    data_divide_ratio: 0.1                        # Split dataset into training and validation sets by this ratio\n\n    shift: 'per_atom_energy_mean'                 # One of 'per_atom_energy_mean*', 'elemwise_reference_energies', float\n    scale: 'force_rms'                            # One of 'force_rms*', 'per_atom_energy_std', 'elemwise_force_rms', float\n\n    # ase.io.read readable data files or structure_list or .sevenn_data files can be used as dataset.\n    # .sevenn_data is preprocessed data set has edges connected (can be obtained by using sevenn_graph_build or by save_** options below)\n    data_format: 'ase'                            # One of 'ase', 'structure_list' (.sevenn_data is always readable)\n    data_format_args:                             # if `data_format` is `ase`, args will be passed to `ase.io.read`\n        index: ':'                                # see `https://wiki.fysik.dtu.dk/ase/ase/io/io.html` for more valid arguments\n\n    # If only load_dataset_path is provided, train/valid set is automatically decided by splitting dataset by divide ratio\n    # If both load_dataset_path &amp; load_validset_path is provided, use load_dataset_path as training set.\n    load_dataset_path: ['../data/train.extxyz']   # Example of using ase as data_format, support multiple datasets and expansion(*)\n    #load_validset_path: ['./valid.sevenn_data']\n\n    #save_dataset_path: './total'                 # Save the preprocessed (in load_dataset_path) dataset\n    #save_by_train_valid: True                    # Save the preprocessed train.sevenn_data, valid.sevenn_data\n</code></pre>"},{"location":"sevenn_train_param/#fine_tune","title":"fine_tune","text":"<pre><code># Example input.yaml for fine-tuning sevennet-0\n# '*' signifies default. You can check log.sevenn for defaults.\n\nmodel:  # model keys should be consistent except for train_* keys\n    chemical_species: 'Auto'\n    cutoff: 5.0\n    channel: 128\n    is_parity: False\n    lmax: 2\n    num_convolution_layer: 5\n    irreps_manual:\n        - \"128x0e\"\n        - \"128x0e+64x1e+32x2e\"\n        - \"128x0e+64x1e+32x2e\"\n        - \"128x0e+64x1e+32x2e\"\n        - \"128x0e+64x1e+32x2e\"\n        - \"128x0e\"\n\n    weight_nn_hidden_neurons: [64, 64]\n    radial_basis:\n        radial_basis_name: 'bessel'\n        bessel_basis_num: 8\n    cutoff_function:\n        cutoff_function_name: 'XPLOR'\n        cutoff_on: 4.5\n    self_connection_type: 'linear'\n\n    train_shift_scale: False   # customizable (True | False)\n    train_denominator: False   # customizable (True | False)\n\ntrain:  # Customizable\n    random_seed: 1\n    is_train_stress: True\n    epoch: 100\n\n    optimizer: 'adam'\n    optim_param:\n        lr: 0.004\n    scheduler: 'exponentiallr'\n    scheduler_param:\n        gamma: 0.99\n\n    force_loss_weight: 0.1\n    stress_loss_weight: 1e-06\n\n    per_epoch: 10  # Generate checkpoints every this epoch\n\n    # ['target y', 'metric']\n    # Target y: TotalEnergy, Energy, Force, Stress, Stress_GPa, TotalLoss\n    # Metric  : RMSE, MAE, or Loss\n    error_record:\n        - ['Energy', 'RMSE']\n        - ['Force', 'RMSE']\n        - ['Stress', 'RMSE']\n        - ['TotalLoss', 'None']\n\n    continue:\n        reset_optimizer: True\n        reset_scheduler: True\n        reset_epoch: True\n        checkpoint: 'SevenNet-0_11July2024'\n        # Set True to use shift, scale, and avg_num_neigh from checkpoint (highly recommended)\n        use_statistic_values_of_checkpoint: True\n\ndata:  # Customizable\n    batch_size: 4\n    data_divide_ratio: 0.1\n\n    # ase.io.read readable data files or structure_list or .sevenn_data files can be used as dataset.\n    # .sevenn_data is preprocessed data set has edges connected (can be obtained by using sevenn_graph_build or by save_** options below)\n    data_format: 'ase'                            # One of 'ase', 'structure_list' (.sevenn_data is always readable)\n    data_format_args:                             # if `data_format` is `ase`, args will be passed to `ase.io.read`\n        index: ':'                                # see `https://wiki.fysik.dtu.dk/ase/ase/io/io.html` for more valid arguments\n\n    # If only load_dataset_path is provided, train/valid set is automatically decided by splitting dataset by divide ratio\n    # If both load_dataset_path &amp; load_validset_path is provided, use load_dataset_path as training set.\n    load_dataset_path: ['fine_tune.extxyz']       # Support multiple files and expansion(*)\n    #load_validset_path: ['./valid.sevenn_data']\n\n    #save_dataset_path: './total'                 # Save the preprocessed (in load_dataset_path) dataset\n    #save_by_train_valid: True                    # Save the preprocessed train.sevenn_data, valid.sevenn_data\n</code></pre>"},{"location":"sevenn_train_param/#sevennet-0","title":"sevennet-0","text":"<pre><code># SevenNet-0\nmodel:\n    chemical_species: 'auto'\n    cutoff: 5.0\n    channel: 128\n    is_parity: False\n    lmax: 2\n    num_convolution_layer: 5\n    irreps_manual:\n        - \"128x0e\"\n        - \"128x0e+64x1e+32x2e\"\n        - \"128x0e+64x1e+32x2e\"\n        - \"128x0e+64x1e+32x2e\"\n        - \"128x0e+64x1e+32x2e\"\n        - \"128x0e\"\n\n    weight_nn_hidden_neurons: [64, 64]\n    radial_basis:\n        radial_basis_name: 'bessel'\n        bessel_basis_num: 8\n    cutoff_function:\n        cutoff_function_name: 'XPLOR'\n        cutoff_on: 4.5\n\n    act_gate: {'e': 'silu', 'o': 'tanh'}\n    act_scalar: {'e': 'silu', 'o': 'tanh'}\n\n    conv_denominator: 'avg_num_neigh'\n    train_shift_scale: False\n    train_denominator: False\n    self_connection_type: 'linear'\ntrain:\n    train_shuffle: False\n    random_seed: 1\n    is_train_stress : True\n    epoch: 600\n\n    loss: 'Huber'\n    loss_param:\n        delta: 0.01\n\n    optimizer: 'adam'\n    optim_param:\n        lr: 0.01\n    scheduler: 'linearlr'\n    scheduler_param:\n        start_factor: 1.0\n        total_iters: 600\n        end_factor: 0.0001\n\n    force_loss_weight : 1.00\n    stress_loss_weight: 0.01\n\n    error_record:\n        - ['Energy', 'RMSE']\n        - ['Force', 'RMSE']\n        - ['Stress', 'RMSE']\n        - ['Energy', 'MAE']\n        - ['Force', 'MAE']\n        - ['Stress', 'MAE']\n        - ['Energy', 'Loss']\n        - ['Force', 'Loss']\n        - ['Stress', 'Loss']\n        - ['TotalLoss', 'None']\n\n    per_epoch: 10\n    # continue:\n    #    checkpoint: './checkpoint_last.pth'\n    #    reset_optimizer: False\n    #    reset_scheduler: False\ndata:\n    data_shuffle: False\n    batch_size: 128  # per GPU batch size, as the model trained with 32 GPUs, the effective batch size equals 4096.\n    scale: 'per_atom_energy_std'\n    shift: 'elemwise_reference_energies'\n\n    data_format: 'ase'\n    save_by_train_valid: False\n    load_dataset_path: [\"path_to_MPtrj_total.sevenn_data\"]\n    load_validset_path: [\"validaset.sevenn_data\"]\n</code></pre>"},{"location":"sevenn_train_param/#full-parameters","title":"Full parameters","text":"<pre><code># Example input.yaml for training SevenNet.\n# The underlying model is nequip (https://github.com/mir-group/nequip), but the names of hyperparameters might different.\n# Defaults model parameter that works well of channel, lmax, and num_convolution_layer are 32, 1, 3 respectively.\n# '*' signifies default. You can check log.sevenn.\n\nmodel:\n    chemical_species: 'Auto'                      # Chemical elements present in the dataset, guess them from load_dataset data if 'auto'\n    cutoff: 5.0                                   # Cutoff radius in Angstroms. If two atoms are within the cutoff, they are connected.\n    channel: 4                                    # The multiplicity(channel) of node features.\n    lmax: 1                                       # Maximum order of irreducible representations (rotation order).\n    num_convolution_layer: 4                      # The number of message passing layers.\n\n    #irreps_manual:                               # Manually set irreps of the model in each layer\n        #- \"128x0e\"\n        #- \"128x0e+64x1e+32x2e\"\n        #- \"128x0e+64x1e+32x2e\"\n        #- \"128x0e+64x1e+32x2e\"\n        #- \"128x0e+64x1e+32x2e\"\n        #- \"128x0e\"\n\n    weight_nn_hidden_neurons: [64, 64]            # Hidden neurons in convolution weight neural network\n    radial_basis:                                 # Function and its parameters to encode radial distance\n        radial_basis_name: 'bessel'               # Only 'bessel' is currently supported\n        bessel_basis_num: 8\n    cutoff_function:                              # Envelop function, multiplied to radial_basis functions to init edge featrues\n        cutoff_function_name: 'poly_cut'          # {'poly_cut' and 'poly_cut_p_value'} or {'XPLOR' and 'cutoff_on'}\n        poly_cut_p_value: 6\n\n    act_gate: {'e': 'silu', 'o': 'tanh'}          # Equivalent to 'nonlinearity_gates' in nequip\n    act_scalar: {'e': 'silu', 'o': 'tanh'}        # Equivalent to 'nonlinearity_scalars' in nequip\n\n    is_parity: False                              # Pairy True (E(3) group) or False (to SE(3) group)\n\n    self_connection_type: 'nequip'                # Default is 'nequip'. 'linear' is used for SevenNet-0.\n\n    conv_denominator: \"avg_num_neigh\"             # Valid options are \"avg_num_neigh*\", \"sqrt_avg_num_neigh\", or float\n    train_shift_scale: False                      # Enable training for shift &amp; scale in output layer\n    train_denominator: False                      # Enable training for denominator in convolution layer\n\ntrain:\n    random_seed: 1\n    is_train_stress: True                         # Includes stress in the loss function\n    epoch: 10                                     # Ends training after this number of epochs\n\n    #loss: 'Huber'                                # Default is 'mse' (mean squared error)\n    #loss_param:\n        #delta: 0.01\n\n    # Each optimizer and scheduler have different available parameters.\n    # You can refer to sevenn/train/optim.py for supporting optimizer &amp; schedulers\n    optimizer: 'adam'                             # Options available are 'sgd', 'adagrad', 'adam', 'adamw', 'radam'\n    optim_param:\n        lr: 0.005\n    scheduler: 'exponentiallr'                    # One of 'steplr', 'multisteplr', 'exponentiallr', 'cosineannealinglr', 'reducelronplateau', 'linearlr'\n    scheduler_param:\n        gamma: 0.99\n\n    force_loss_weight: 0.1                        # Coefficient for force loss\n    stress_loss_weight: 1e-06                     # Coefficient for stress loss (to kbar unit)\n\n    per_epoch:  5                                # Generate checkpoints every this epoch\n\n    # TotalEnergy, Energy, Force, Stress, Stress_GPa, TotalLoss\n    # RMSE, MAE, or Loss\n    error_record:\n        - ['Energy', 'RMSE']\n        - ['Force', 'RMSE']\n        - ['Stress', 'RMSE']\n        - ['TotalLoss', 'None']\n\n    # Continue training model from given checkpoint, or pre-trained model checkpoint for fine-tuning\n    #continue:\n        #reset_optimizer: False                    # Set True for fine-tuning\n        #reset_scheduler: False                    # Set True for fine-tuning\n        #checkpoint: 'checkpoint_best.pth'         # Checkpoint of pre-trained model or a model want to continue training.\n        #use_statistic_values_of_checkpoint: False # Set True to use shift, scale, and avg_num_neigh from checkpoint or not\n\n    # If the dataset changed (for fine-tuning),\n    # setting 'use_statistic_value_of_checkpoint' to True roughly changes model's accuracy in the beginning of training.\n    # We recommand to use it as False, and turn train_shift_scale and train_avg_num_neigh to True.\n\ndata:\n    batch_size: 2                                 # Per GPU batch size.\n    data_divide_ratio: 0.1                        # Split dataset into training and validation sets by this ratio\n\n    #shift: 'per_atom_energy_mean'                # One of 'per_atom_energy_mean*', 'elemwise_reference_energies', float\n    #scale: 'force_rms'                           # One of 'force_rms*', 'per_atom_energy_std', 'elemwise_force_rms', float\n\n    # ase.io.read readable data files or structure_list or .sevenn_data files can be used as dataset.\n    # .sevenn_data is preprocessed data set has edges connected (can be obtained by using sevenn_graph_build or by save_** options below)\n    data_format: 'ase'                   # Default is 'ase'. Choices are 'ase', 'structure_list', '.sevenn_data'\n    data_format_args:                            # Paramaters, will be passed to ase.io.read\n        energy_key: 'TotEnergy'                  # Key for energy in extxyz file\n        force_key: 'force'                       # Key for force in extxyz file\n\n    # ASE tries to infer its type by extension, in this case, extxyz file is loaded by ase.\n    #load_dataset_path: ['../data/test.extxyz']   # Example of using ase as data_format\n\n    # If only load_dataset_path is provided, train/valid set is automatically decided by splitting dataset by divide ratio\n    # If both load_dataset_path &amp; load_validset_path is provided, use load_dataset_path as training set.\n    load_dataset_path: ['./1_process_data/dataset_1593.xyz']\n    #load_validset_path: ['./valid.sevenn_data']\n\n    #save_dataset_path: 'total'                   # Save the preprocessed (in load_dataset_path) dataset\n    #save_by_train_valid: True                    # Save the preprocessed train.sevenn_data, valid.sevenn_data\n    #save_by_label: False                         # Save the dataset by labels specified in the structure_list\n</code></pre>"}]}